{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatdataset():\n",
    "    dataset = [['sunny','high','false','hot','yes'],\n",
    "               ['sunny','high','false','hot','yes'],\n",
    "               ['sunny','high','false','hot','no'],\n",
    "               ['sunny','high','false','hot','no'],\n",
    "               ['sunny','high','true','mild','no'],\n",
    "               ['overcast','high','false','mild','yes'],\n",
    "               ['overcast','normal','false','mild','yes'],\n",
    "               ['overcast','normal','false','mild','yes'],\n",
    "               ['overcast','normal','false','mild','yes'],\n",
    "               ['rainy','normal','true','cool','yes'],\n",
    "               ['rainy','normal','true','cool','yes'],\n",
    "               ['rainy','normal','true','cool','yes'],\n",
    "               ['rainy','high','true','mild','no'],\n",
    "               ['rainy','normal','true','cool','no']]\n",
    "    \n",
    "    labels = ['outlook','hunidity','windy','temperaturn']\n",
    "    return dataset,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_entropy(dataset):\n",
    "    '''计算数据集的信息熵，度量数据集的无序程度'''\n",
    "    m = len(dataset)   # 一共有多少数据\n",
    "    labels_count = {}  # 创建一个字典用于统计每一类的数据个数\n",
    "    for data in dataset:   \n",
    "        label = data[-1]                                    #将数据集按照标签进行统计，\n",
    "        labels_count[label] = labels_count.get(label,0)+1   #结果存为字典\n",
    "                                                                \n",
    "    entropy = 0.0\n",
    "    for key in labels_count.keys():                         #计算信息熵\n",
    "        prob = float(labels_count[key])/m\n",
    "        entropy -= prob*np.log(prob) \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset,axis,values):                      \n",
    "    '''划分数据集dataset,axis为划分的依据特征,values为需要返回的特征值'''\n",
    "    reduce_dataset =[]                                  \n",
    "    for data in dataset:                        \n",
    "        if data[axis]==values:                  #按照轴axis处取值的不同划分为两类\n",
    "            reduce_data = data[:axis]           #等于value的一类，否则为另一类\n",
    "            reduce_data.extend(data[axis+1:])\n",
    "            reduce_dataset.append(reduce_data)\n",
    "    return reduce_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_split_feature(dataset):\n",
    "    '''选择最好的特征来划分数据集'''\n",
    "    feature_num = len(dataset[0])-1\n",
    "    base_entropy = caculate_entropy(dataset)     #计算原始数据集的信息熵\n",
    "    print('base_entropy = ',base_entropy)\n",
    "    best_info_gain = 0.0;\n",
    "    best_feature = -1\n",
    "    \n",
    "    for i in range(feature_num):\n",
    "        #对每一个特征进行划分数据集，计算他们的信息增益，取信息增益最大的那个作为最好特征\n",
    "        feature_list = [data[i] for data in dataset]\n",
    "        unique_feature = set(feature_list)\n",
    "        \n",
    "        new_entropy = 0.0\n",
    "        for feature in unique_feature:           #对每一个特征\n",
    "            sub_dataset = split_dataset(dataset,i,feature)\n",
    "            prob = len(sub_dataset)/float(len(dataset))\n",
    "            new_entropy += prob*caculate_entropy(sub_dataset)\n",
    "        print('new = ',new_entropy)\n",
    "        info_gain = base_entropy-new_entropy\n",
    "        print('info_gain = ',info_gain)                         #获得信息增益\n",
    "        \n",
    "        if (info_gain>best_info_gain):          #选出获得最大信息增益的特征\n",
    "            best_info_gain = info_gain\n",
    "            best_feature = i\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_entropy =  0.6517565611726531\n",
      "new =  0.48072261929232607\n",
      "info_gain =  0.17103394188032706\n",
      "new =  0.5465122114944403\n",
      "info_gain =  0.10524434967821283\n",
      "new =  0.6183974457364384\n",
      "info_gain =  0.033359115436214726\n",
      "new =  0.6315010221774208\n",
      "info_gain =  0.02025553899523236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, labels = creatdataset()\n",
    "choose_split_feature(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6517565611726531"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9/14*np.log(9/14)+5/14*np.log(5/14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6517565611726531"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caculate_entropy(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
